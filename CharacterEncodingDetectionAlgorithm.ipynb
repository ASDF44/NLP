{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharacterEncodingDetectionAlgorithm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ASDF44/NLP/blob/master/CharacterEncodingDetectionAlgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yhmzVTANIGo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Character Encoding Detection Algorithm\n",
        "\n",
        "\n",
        "Computers and humans do not communicate the same way. The language we type in, it maybe English, Russian or Japanese, the computer perceives them all in terms of binary codes. \n",
        "The process of converting the written scripts into machine readable binary codes is called character encoding. As with any problem, it is initially simple, but as we dig into it \n",
        "deeper, the problem becomes complex. The complexity of character encoding increased when we wanted to access the computers using languages other than English. \n",
        "The initial encoding system, Ipresume is the ASCII encoding system. ASCII encoding is a 7-bit binary encoding system. \n",
        "\n",
        "Character ================ Some Encoding ================> Machine readable code\n",
        "\n",
        "With need of more bits to encode UTF-8 coding came into existence. UTF-8 consists of 8 bits instead of 7 bits, which makes 1 byte. This is the reason why it is also called as \n",
        "byte encoding system. Then with the further encompassment of other regional languages the need to expand 1 byte to byte was needed, and further increased to 3, 4 bytes. \n",
        "\n",
        "\n",
        "Character Encoding Detection--\n",
        "\n",
        "First part of detecting the encoding technique, we first need to check the BOM (Byte Order Mark) of the file. The BOM decides whether the encoding is little endian or big endian. \n",
        "But incase of UTF-8 we don’t need BOM. UTF-8 is very easy to detect and reliable too. It has a very low chance of false positives. Even if the text is valid UTF-8 text, if it \n",
        "ranges in 0-127 then it will be considered to be ASCII encoding, but if the characters also enter the 128-191 range then it is considered to be UTF-8 encoding. \n",
        "For UTF-16, we have two methods. \n",
        "Method-1\n",
        "After checking the text is not UTF-8, we go further to check 2 byte code. First, we check the endianness of the text. Check for a character, if the characters are 0x0a0x0b then \n",
        "they can be encoded as both, 0x0a0x0b and 0x0b0x0a. So if every character shows this same pattern, then they can be considered as UTF-16 encoding. \n",
        "Method-2\n",
        "Just scan the text file for even positioned “00” or odd positioned “00”. If the text has sufficient number of “00” in it, then it can be considered to be UTF-16 encoding. \n",
        "\n",
        "\n",
        "Detection of encoding is still pretty difficult, because with different languages comes different encodings, but all these encodings use the same binary system, and ensuring no \n",
        "overlap is not possible. Therefore, we need algorithms to detect the encoding technique. \n",
        "\n",
        "A suitable system of encoding seems to be the region-based encoding technique. The detection can be simplified by using the region. Say if the region of the text file is France, \n",
        "then the text has a high probability of being French, and thus we have narrowed our search down to French to machine readable code technique. \n",
        "\n",
        "So far it can be concluded that UTF-8 detection is the most reliable detection possible. The UTF-16 detection is also somewhat reliable.This is due to the large percentage of \n",
        "invalid byte sequences in UTF-8, so that text in any other encoding that uses bytes with the high bit set is extremely unlikely to pass a UTF-8 validity test. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnr9j8kMNZQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}