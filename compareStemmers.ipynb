{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sing', 'lie', 'tri', 'affection']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As far as i knoit doe\n",
    "from nltk.stem import PorterStemmer\n",
    "sp = PorterStemmer()\n",
    "[sp.stem ('sing'), sp.stem ('lying'), sp.stem('tries'), sp.stem('affectionately')]\n",
    "# Now porter stemmer is a very powerful stemmer in many cases, \n",
    "# but as per it's algorithm it does not use a lexicon but instead a \n",
    "# beautiful analogy that it just removes the suffix only from words that\n",
    "# have atleast one syllable eg. - sing does not give s in porter stemmer \n",
    "# instead it returns sing!!! And I don't know how but surprisingly it\n",
    "# returns lie for lying which does not fit into the algorithm's pattern\n",
    "# but at the same time it fails in case of a simple word like affection \n",
    "# and instead of returning affect it returns affection back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "sre = RegexpStemmer('ing')\n",
    "sre.stem('singing')\n",
    "# now regex stemmer seems utterly useless coz it \n",
    "# will return a word-regexstring which would prove silly\n",
    "# in case of words that actually end with it like\n",
    "# sing for regexstem=-ing will return s which is not correct, but \n",
    "# the regexpstemmer can prove useful in cases like when the word has\n",
    "# multiple suffixes like affectionate, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "frenchstemmer = SnowballStemmer('french')\n",
    "len(SnowballStemmer.languages)\n",
    "# Now this stemmer is a living proof that there are people\n",
    "# who care about natural language processing, It covers 16 language\n",
    "# stemmings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buvon'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchstemmer.stem('buvons')\n",
    "# buvons actual root word is boire(to drink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer.languages\n",
    "# A very sad thing about snowball stemmer is, that it does not have\n",
    "# any Indian language in the corpus, not even Hindi or Sanskrit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ox'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('oxen')\n",
    "# I can definitely say that lemmatizer uses a lexicon to find the lemma\n",
    "# or atleast an enormous fsa, coz getting the lemma is extremely difficult\n",
    "# in cases like gesse->goose or mice->mouse, etc. and again the lemma has\n",
    "# data pos tagged coz it definitely aids in finding the lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('best', pos='a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better', pos = 'a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('worse', pos = 'a') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('worst', pos = 'a') \n",
    "# for better we get good but for best we get best and in \n",
    "# which means superlative is out of scope, but no it is just an anecdotal evidence and we as a CS people reject it\n",
    "# So, after testing it for bad, worse, worst we can conclude that our earlier inference was wrong coz the lemmatizer works perfectly\n",
    "# here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affectionsately'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('affectionsately', pos='a')\n",
    "# should give affection but unable to give affection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out glove by Stanford\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vetorizer.fit_transform(corpus)\n",
    "# vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
